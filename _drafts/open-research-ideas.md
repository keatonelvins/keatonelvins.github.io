---
layout: post
title: vector-based random matrix adaption
image: true
---

Current paradigm: say we hit the data wall, all models are eventually seeing the same training data and we reach some massive size/computational ability.
I think that the ability of this pre-trained model is HUGE. However, there is a lot of work to be done in post-training to end up with a truly great model.

Paper that claims models are converging in terms of how they represent the world: https://arxiv.org/pdf/2405.07987

Things to think about:
We have a massive pretrained model trained on trillions of high-quality tokens
- Can the model express uncertainty still?
